{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"spacesavers2","text":""},{"location":"#background","title":"Background","text":"<p><code>spacesavers2</code></p> <ul> <li>crawls through the provided folder (and its subfolders),</li> <li>gathers stats for each file like size, inode, user/group information, etc.,</li> <li>calculates unique hashes for each file,</li> <li>using the information gathers determines \"duplicates\",</li> <li>reports \"high-value\" duplicates, i.e., the ones that will give back most diskspace, if deleted,and</li> <li>makes a \"counts-matrix\" style matrix with folders as rownames and users a columnnames with each cell representing duplicate bytes.</li> </ul> <p>New improved parallel implementation of <code>spacesavers</code>. <code>spacesavers</code> is soon to be decommissioned!</p> <p>Note: <code>spacesavers2</code> requires python version 3.11 or later and the xxhash library. These dependencies are already installed on biowulf (as a conda env). The environment for running <code>spacesavers2</code> can get set up using:</p> <pre><code>. \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" &amp;&amp; \\\nconda activate py311\n</code></pre>"},{"location":"#commands","title":"Commands","text":"<p><code>spacesavers2</code> has the following Basic commands:</p> <ul> <li>spacesavers2_catalog</li> <li>spacesavers2_mimeo</li> <li>spacesavers2_grubbers</li> <li>spacesavers2_usurp</li> <li>spacesavers2_e2e</li> <li>spacesavers2_pdq</li> </ul>"},{"location":"#use-case","title":"Use case","text":"<p>One would like to monitor the per-user digital footprint on shared data drives like <code>/data/CCBR</code> on biowulf. Setting the <code>spacesavers2_e2e</code> as a weekly cronjob will allow automation of this task. <code>slurm_job</code> script is also provided to work as a template for using the job scheduler on the HPC to submit (possibly, as cronjob).</p>"},{"location":"catalog/","title":"catalog","text":""},{"location":"catalog/#spacesavers2_catalog","title":"spacesavers2_catalog","text":"<p>This uses <code>glob</code> library to list all files in a user-provided folder recursively. </p> <p>For each file it gathers information like:  - filesize  - check if it is symlink  - inode  - creation and modificatino time  - user id  - group id</p> <p>For each file it also computes a unique hash (using xxHash library) for:  - top chunk of the file  - bottom chunk of the file (<code>-e</code> option)</p>"},{"location":"catalog/#inputs","title":"Inputs","text":"<ul> <li><code>--folder</code>: Path to the folder to run <code>spacesavers2_catalog</code> on.</li> <li><code>--threads</code>: <code>spacesavers2_catalog</code> uses multiprocessing library to parallelize orchestration. This defines the number of threads to run in parallel.</li> <li><code>--buffersize</code>: This defines the size of the top (or bottom) chunk of the file to be used to by xxHash. (default 128 KB)</li> <li><code>--ignoreheadersize</code>: This defines the size of the top of the file to be ignored before reading the top chunk for xxHash calculation. This is useful for ignore the header portion of files like BAM or BIGWIG which may have the same top chunk (Eg. samples aligned to the same reference index will generate BAMs with the same header hence original <code>spacesaver</code>, which only looks at the top chunk, may call them duplicates. <code>spacesavers2</code> tries to do a better job at this.). (default 1 MB)</li> <li><code>--se</code>: Comma-separated special extensions for home <code>spacesavers2</code> ignores the headers before extracting the top chunk for xxHash calculation. The default is \"bam,bai,bigwig,bw,csi\".</li> <li><code>--bottomhash</code>: Default False. Use the bottom chunk along with the top chunk of the file for xxHash calculation.</li> <li><code>--outfile</code>: If not supplied then the optput is written to the screen.</li> <li><code>--brokenlink</code>: Default False. Create a file listing broken symlinks per-user.</li> <li><code>--geezer</code>: Default False. Create a file listing really old files per-user. Output files have 3 columns: age, size, path</li> <li><code>--geezerage</code>: Default 5 * 365. age in days to be considered a geezer file.</li> <li><code>--geezersize</code>: Default 10 MiB. minimum size in bytes of geezer file to be reported.</li> </ul> <p>NOTE: <code>spacesavers2_catalog</code> reports errors (eg. cannot read file) to STDERR</p> <pre><code>usage: spacesavers2_catalog [-h] -f FOLDER [-p THREADS] [-b BUFFERSIZE] [-i IGNOREHEADERSIZE] [-x SE] [-s ST_BLOCK_BYTE_SIZE] [-o OUTFILE]\n                            [-e | --bottomhash | --no-bottomhash] [-l | --brokenlink | --no-brokenlink] [-g | --geezers | --no-geezers]\n                            [-a GEEZERAGE] [-z GEEZERSIZE] [-v]\n\nspacesavers2_catalog: get per file info.\n\noptions:\n  -h, --help            show this help message and exit\n  -f FOLDER, --folder FOLDER\n                        spacesavers2_catalog will be run on all files in this folder and its subfolders\n  -p THREADS, --threads THREADS\n                        number of threads to be used (default 4)\n  -b BUFFERSIZE, --buffersize BUFFERSIZE\n                        buffersize for xhash creation (default=128 * 1028 bytes)\n  -i IGNOREHEADERSIZE, --ignoreheadersize IGNOREHEADERSIZE\n                        this sized header of the file is ignored before extracting buffer of buffersize for xhash creation (only for special\n                        extensions files) default = 1024 * 1024 * 1024 bytes\n  -x SE, --se SE        comma separated list of special extensions (default=bam,bai,bigwig,bw,csi)\n  -s ST_BLOCK_BYTE_SIZE, --st_block_byte_size ST_BLOCK_BYTE_SIZE\n                        st_block_byte_size on current filesystem (default 512)\n  -o OUTFILE, --outfile OUTFILE\n                        outfile ... catalog file .. by default output is printed to screen\n  -e, --bottomhash, --no-bottomhash\n                        separately calculated second hash for the bottom/end of the file.\n  -l, --brokenlink, --no-brokenlink\n                        output per-user broken links list.\n  -g, --geezers, --no-geezers\n                        output per-user geezer files list.\n  -a GEEZERAGE, --geezerage GEEZERAGE\n                        age in days to be considered a geezer file (default 5yrs ... 5 * 365).\n  -z GEEZERSIZE, --geezersize GEEZERSIZE\n                        minimum size in bytes of geezer file to be reported (default 10MiB ... 10 * 1024 * 1024).\n  -v, --version         show program's version number and exit\n\nVersion:\n    v0.11.4\nExample:\n    &gt; spacesavers2_catalog -f /path/to/folder -p 56 -e -l -g\n</code></pre>"},{"location":"catalog/#output","title":"Output","text":""},{"location":"catalog/#catalog-file","title":"catalog file","text":"<p><code>spacesavers2_catalog</code> creates one semi-colon seperated output line per input file. Here is an example line:</p> <p><pre><code>% head -n1 test.catalog\n\"/data/CBLCCBR/kopardevn_tmp/spacesavers2_testing/_data_CCBR_Pipeliner_db_PipeDB_Indices.ls.old\";False;1653453;47;372851499;1;1;5;5;37513;57886;4707e661a1f3beca1861b9e0e0177461;52e5038016c3dce5b6cdab635765cc79;\n</code></pre> The 13 items in the line are as follows:</p> Column Description Example 1 File absolute path \"/data/CBLCCBR/kopardevn_tmp/spacesavers2_testing/_data_CCBR_Pipeliner_db_PipeDB_Indices.ls.old\" 2 Is file a symlink? FALSE 3 file size in bytes 1653453 4 file device 47 5 file inode 372851499 6 number of hardlinks 1 7 access age in days 1 8 modification age in days 5 9 creation age in days 5 10 user id 37513 11 group id 57886 12 top chunk xxHash 4707e661a1f3beca1861b9e0e0177461 13 bottom chunk xxHash 52e5038016c3dce5b6cdab635765cc79 <p>NOTE: Some files may have \";\" or spaces in their name, hence adding quotes around the absolute file path.</p>"},{"location":"catalog/#broken-links-file","title":"broken links file","text":"<ul> <li>simply lists the paths which are symbolic links, but the destination files do not exist anymore!</li> <li>one file per username.</li> </ul> <p>DISCLAIMER:  - may contain false-positives if the user running <code>spacesavers2_catalog</code> does not have read access to the symlinks destination</p>"},{"location":"catalog/#geezer-file","title":"geezer file","text":"<ul> <li>lists really old files (default &gt; 5 years).</li> <li>list has 3 columns: age, size and path.</li> <li>one file per username.</li> </ul>"},{"location":"e2e/","title":"e2e","text":""},{"location":"e2e/#spacesavers2_e2e","title":"spacesavers2_e2e","text":"<p>This is a wrapper to run all the <code>spacesavers2</code> commands in the correct order. It automatically:</p> <ul> <li>adds appropriate prefixes to output files (including time)</li> <li>catalogs files in the folder provided</li> <li>finds duplicates</li> <li>finds high-value duplicates</li> <li>creates blamematrix</li> </ul> <p>This is ideal wrapper to be added as a cronjob.</p> <pre><code> % spacesavers2_e2e --help\nusage: spacesavers2_e2e [-h] -i INFOLDER [-p THREADS] [-q QUOTA] -o OUTFOLDER\n\nEnd-to-end run of spacesavers2\n\noptions:\n  -h, --help            show this help message and exit\n  -f FOLDER, --folder FOLDER\n                        Folder to run spacesavers_catalog on.\n  -p THREADS, --threads THREADS\n                        number of threads to use\n  -d MAXDEPTH, --maxdepth MAXDEPTH\n                        maxdepth for mimeo\n  -l LIMIT, --limit LIMIT\n                        limit for running spacesavers_grubbers\n  -q QUOTA, --quota QUOTA\n                        total size of the volume (default = 200 for /data/CCBR)\n  -o OUTFOLDER, --outfolder OUTFOLDER\n                        Folder where all spacesavers_e2e output files will be saved\n</code></pre>"},{"location":"grubbers/","title":"grubbers","text":""},{"location":"grubbers/#spacesavers2_grubbers","title":"spacesavers2_grubbers","text":"<p>This takes in the <code>mimeo.files.gz</code> generated by <code>spacesavers2_mimeo</code> and processes it to:</p> <ul> <li>sort duplicates by total size</li> <li>reports the \"high-value\" duplicates.</li> </ul> <p>Deleting these high-value duplicates first will have the biggest impact on the users overall digital footprint.</p>"},{"location":"grubbers/#inputs","title":"Inputs","text":"<ul> <li><code>--filesgz</code> output file from <code>spacesavers2_mimeo</code>.</li> <li><code>--limit</code> lower cut-off for output display (default 5 GiB). This means that duplicates with overall size of less than 5 GiB will not be displayed. Set 0 to report all.</li> </ul> <pre><code>\u2570\u2500\u25cb spacesavers2_grubbers --help\nspacesavers2_grubbers:00000.00s:version: v0.10.2-dev\nusage: spacesavers2_grubbers [-h] -f FILESGZ [-l LIMIT] [-o OUTFILE] [-v]\n\nspacesavers2_grubbers: get list of large duplicates sorted by total size\n\noptions:\n  -h, --help            show this help message and exit\n  -f FILESGZ, --filesgz FILESGZ\n                        spacesavers2_mimeo prefix.&lt;user&gt;.mimeo.files.gz file\n  -l LIMIT, --limit LIMIT\n                        stop showing duplicates with total size smaller than (5 default) GiB. Set 0 for unlimited.\n  -o OUTFILE, --outfile OUTFILE\n                        output tab-delimited file (default STDOUT)\n  -v, --version         show program's version number and exit\n\nVersion:\n    v0.10.2-dev\nExample:\n    &gt; spacesavers2_grubbers -f /output/from/spacesavers2_finddup/prefix.files.gz\n</code></pre>"},{"location":"grubbers/#outputs","title":"Outputs","text":"<p>The output is displayed on STDOUT and is tab-delimited with these columns:</p> Column Description 1 combined hash 2 number of duplicates found 3 total size of all duplicates (human readable) 4 size of each duplicate (human readable) 5 original file 6 \";\"-separated list of duplicates files <p>Here is an example output line:</p> <pre><code>183e9dc341073d9b75c817f5ed07b9ac#183e9dc341073d9b75c817f5ed07b9ac   5   0.07 KiB    0.01 KiB    \"/data/CCBR/abdelmaksoudaa/test/a\"  \"/data/CCBR/abdelmaksoudaa/test/b\";\"/data/CCBR/abde\nlmaksoudaa/test/c\";\"/data/CCBR/abdelmaksoudaa/test/d\";\"/data/CCBR/abdelmaksoudaa/test/e\";\"/data/CCBR/abdelmaksoudaa/test/f\"\n</code></pre> <p><code>spacesavers2_grubbers</code> is typical used to find the \"low-hanging\" fruits ... aka ... the \"high-value\" duplicates which need to be deleted first to quickly have the biggest impact on the users overall digital footprint.</p>"},{"location":"mimeo/","title":"mimeo","text":""},{"location":"mimeo/#spacesavers2_mimeo","title":"spacesavers2_mimeo","text":"<p>This takes in the <code>catalog</code> file generated by <code>spacesavers2_catalog</code> and processes it to:</p> <ul> <li>find duplicates</li> <li>create per-user summary reports for each user (and all users).</li> </ul>"},{"location":"mimeo/#inputs","title":"Inputs","text":"<ul> <li><code>--catalog</code> is the output file from  <code>spacesavers2_catalog</code>. Thus, <code>spacesavers2_catalog</code> needs to be run before running <code>spacesavers2_mimeo</code>.</li> <li><code>--maxdepth</code> maximum folder depth upto which reports are aggregated</li> <li><code>--outdir</code> path to the output folder</li> <li><code>--prefix</code> prefix to be added to the output file names eg. date etc.</li> <li><code>--duplicatesonly</code> only report duplicates in the <code>.files.gz</code> output files. This saves a lot of disc space. (Highly recommended!)</li> <li><code>--quota</code> defines the size of the overall file mount. (eg. 200 TB for <code>/data/CCBR</code> on BIOWULF.) OccScore is dependent on this and should be provided appropriately for accurate results.</li> </ul> <pre><code>% spacesavers2_mimeo --help\nusage: spacesavers2_mimeo [-h] -f CATALOG [-d MAXDEPTH] [-o OUTDIR] [-p PREFIX] [-q QUOTA] [-z | --duplicatesonly | --no-duplicatesonly] [-k | --kronaplot | --no-kronaplot] [-v]\n\nspacesavers2_mimeo: find duplicates\n\noptions:\n  -h, --help            show this help message and exit\n  -f CATALOG, --catalog CATALOG\n                        spacesavers2_catalog output from STDIN or from catalog file\n  -d MAXDEPTH, --maxdepth MAXDEPTH\n                        folder max. depth upto which reports are aggregated ... absolute path is used to calculate depth (Default: 10)\n  -o OUTDIR, --outdir OUTDIR\n                        output folder\n  -p PREFIX, --prefix PREFIX\n                        prefix for all output files\n  -q QUOTA, --quota QUOTA\n                        total quota of the mount eg. 200 TB for /data/CCBR\n  -z, --duplicatesonly, --no-duplicatesonly\n                        Print only duplicates to per user output file.\n  -k, --kronaplot, --no-kronaplot\n                        Make kronaplots for duplicates.(ktImportText must be in PATH!)\n  -v, --version         show program's version number and exit\n\nVersion:\n    v0.10.2-dev\nExample:\n    &gt; spacesavers2_mimeo -f /output/from/spacesavers2_catalog -o /path/to/output/folder -d 7 -q 10 -k\n</code></pre>"},{"location":"mimeo/#outputs","title":"Outputs","text":"<p>After completion of run, <code>spacesavers2_mimeo</code> creates <code>*.mimeo.files.gz</code> (list of files per user + one \"allusers\" file) and <code>.summary.txt</code> (overall stats at various depths) files in the provided output folder. if <code>-k</code> is provided (and ktImportText from kronatools is in PATH) then krona specific TSV and HTML pages are also generated. It also generates a <code>blamematrix.tsv</code> file with folders on rows and users on columns with duplicate bytes per-folder-per-user. This file can be used to create a \"heatmap\" to pinpoint folder with highest duplicates overall as well as on a per-user basis.</p> <p>Here are the details:</p>"},{"location":"mimeo/#duplicates","title":"Duplicates","text":"<p><code>spacesavers2_mimeo</code> uses the following logic to find duplicates:</p> <ul> <li>Bin files by their top (and bottom) xxHashes irrespective of user id (allusers mode)</li> <li>Check if each bin has unique sized files. If a bin has more than 1 size, then it needs to be binned further. Sometimes, xxHash of top and bottom chunks also gives the same combination of hash for differing files. These files will have different sizes. Hence, re-bin them accordingly.</li> <li>If same size, then check inodes. If all files in the same bin have the same inode, then these are just hard-links. But, if there are multiple inodes, then we have duplicates!</li> <li>If we have duplicates, then <code>spacesavers2_mimeo</code> keeps track of number of duplicates per bin. Number of duplicates is equal to number of inodes in each bin minus one.</li> <li>If we have duplicates, then the oldest file is identified and considered to be the original file. All other files are marked duplicate, irrespective of user id.</li> <li>duplicate files are reported in gzip format with the following columns for all users and per-user basis</li> </ul> <p>Here is what the <code>.files.gz</code> file columns (space-separated) represent:</p> Column Description 1 top chunk and bottom chunk hashes separated by \"#\" 2 separator \":\" 3 Number of duplicates files (not duplicate inodes) 4 Size of each file 5 List of users duplicates serapated by \"##\" <p>NOTE: Number of dupicate files can be greater than number of duplicate inodes as each file can have multiple hard links already. Hence, while calculating total duplicate bytes we use (total_number_of_unique_inodes_per_group_of_duplicate_files - 1) X size_of_each_file. The \"minus 1\" is to not count the size of the original file.</p> <p>Each file in the last column above is \";\" separated with the same 13 items as described in the <code>catalog</code> file. The only difference is that the username and groupame are now appended to each file entry.</p> <p>Along with creating one <code>.mimeo.files.gz</code> and <code>.mimeo.summary.txt</code> file per user encountered, <code>spacesavers2_mimeo</code> also generates a <code>allusers.mimeo.files.gz</code> file for all users combined. This file is later used by <code>spacesavers2_blamematrix</code> as input.</p>"},{"location":"mimeo/#summaries","title":"Summaries","text":"<p>Summaries, files ending with <code>.mimeo.summary.txt</code> are collected and reported for all users (<code>allusers.mimeo.summary.txt</code>) and per-user (<code>USERNAME.mimeo.summary.txt</code>) basis for user-defined depth (and beyond). The columns (tab-delimited) in the summary file:</p> Column Description 1 absolute path 2 total bytes 3 duplicate bytes 4 percent duplicate bytes 5 total files 6 duplicate files 7 percent duplicate files 8 average file age of all files (days) 9 average file age of duplicates (days) 10 AgeScore 11 DupScore 12 OccScore 13 OverallScore <p>For columns 10 through 13, the same logic is used as spacesavers.</p>"},{"location":"mimeo/#kronatsv-and-kronahtml","title":"KronaTSV and KronaHTML","text":"<ul> <li>KronaTSV is tab-delimited with first column showing the number of duplicate bytes and every subsequent column giving the folder depths.</li> <li>ktImportText is then used to convert the KronaTSV to KronaHTML which can be shared easily and only needs a HTML5 supporting browser for viewing.</li> </ul>"},{"location":"mimeo/#blamematrix","title":"Blamematrix","text":"<ul> <li>rows are folders as 1 level deeper than the \"mindepth\"</li> <li>columns are all individual usernames, plus an \"allusers\" column</li> <li>only duplicate-bytes are reported</li> </ul>"},{"location":"pdq/","title":"pdq","text":""},{"location":"pdq/#spacesavers2_pdq","title":"spacesavers2_pdq","text":"<p>pdq = Pretty Darn Quick</p> <p>This uses <code>glob</code> library to list all files in a user-provided folder recursively. </p> <p>For each user it gathers information like:  - total number of inodes  - total number of bytes</p> <p>It is quick tool to gather datapoints to monitor filesystem usage. Typically, can be run once daily and compared with previous days run to find large changes.</p>"},{"location":"pdq/#inputs","title":"Inputs","text":"<ul> <li><code>--folder</code>: Path to the folder to run <code>spacesavers2_pdq</code> on.</li> <li><code>--threads</code>: <code>spacesavers2_pdq</code> uses multiprocessing library to parallelize orchestration. This defines the number of threads to run in parallel.</li> <li><code>--outfile</code>: If not supplied then the output is written to the screen.</li> <li><code>--json</code>: Optional, if provided output is also written in JSON format.</li> </ul> <p>NOTE: <code>spacesavers2_pdq</code> reports errors (eg. cannot read file) to STDERR</p> <pre><code>usage: spacesavers2_pdq [-h] -f FOLDER [-p THREADS] [-o OUTFILE] [-j JSON] [-v]\n\nspacesavers2_pdq: get quick per user info (number of inodes and bytes).\n\noptions:\n  -h, --help            show this help message and exit\n  -f FOLDER, --folder FOLDER\n                        spacesavers2_pdq will be run on all inodes in this folder and its subfolders\n  -p THREADS, --threads THREADS\n                        number of threads to be used (default 4)\n  -o OUTFILE, --outfile OUTFILE\n                        outfile ... by default output is printed to screen\n  -j JSON, --json JSON  outfile file in JSON format.\n  -v, --version         show program's version number and exit\n\nVersion:\n    v0.11.6\nExample:\n    &gt; spacesavers2_pdq -f /path/to/folder -p 4 -o /path/to/output_file\n</code></pre>"},{"location":"pdq/#output","title":"Output","text":""},{"location":"pdq/#tab-delimited-output-file","title":"tab-delimited output (file)","text":"<p><code>spacesavers2_pdq</code> creates one tab seperated output line per user:</p> <p><pre><code>% head -n1 test.out\nuser1       1386138 6089531321856\nuser2  230616  2835680212992\nuser3      1499    126442496\n</code></pre> The 3 items in the line are as follows:</p> Column Description Example 1 username \"user1\" 2 total no. of inodes owned 1386138 3 total no. of bytes occupied 6089531321856"},{"location":"pdq/#json-output","title":"JSON output","text":"<p>Here is an example output:</p> <pre><code>{\n \"/path/to/some/folder   \": {\n  \"1234\": {\n   \"username\": \"user1\",\n   \"ninodes\": 1267,\n   \"nbytes\": 96084992\n  },\n  \"4356\": {\n   \"username\": \"user2\",\n   \"ninodes\": 895,\n   \"nbytes\": 89249280\n  }\n }\n}\n</code></pre> <p><code>spacesavers2_pdq</code> creates TSV (or JSON) file per-datamount per-run (typically per-date). If run daily, this soon creates a lot of files to keep track of. Hence, it is best to save the data in a sqlite db using:</p> <ul> <li><code>spacesavers2_pdq_create_db</code> and</li> <li><code>spacesavers2_pdq_update_db</code></li> </ul>"},{"location":"pdq_create_db/","title":"pdq_create_db","text":""},{"location":"pdq_create_db/#spacesavers2_pdq_create_db","title":"spacesavers2_pdq_create_db","text":"<p>pdq = Pretty Darn Quick</p> <p><code>spacesavers2_pdq</code> creates TSV (or JSON) file per-datamount per-run (typically per-date). If run daily, this soon creates a lot of files to keep track of. Hence, it is best to save the data in a sqlite db. This command create the basic schema for that db. The schema looks like this:</p> <p></p>"},{"location":"pdq_create_db/#inputs","title":"Inputs","text":"<ul> <li><code>--filepath</code>: where to create the \".db\" file.</li> <li><code>--overwrite</code>: toggle to overwrite if the \".db\" file already exists.</li> </ul> <pre><code>usage: spacesavers2_pdq_create_db [-h] -f FILEPATH [-o | --overwrite | --no-overwrite] [-v]\n\nspacesavers2_pdq_create_db: create a sqlitedb file with the optimized schema.\n\noptions:\n  -h, --help            show this help message and exit\n  -f FILEPATH, --filepath FILEPATH\n                        spacesavers2_pdq_create_db will create this sqlitedb file\n  -o, --overwrite, --no-overwrite\n                        overwrite output file if it already exists. Use this with caution as it will delete existing file and its contents!!\n  -v, --version         show program's version number and exit\n\nVersion:\n    v0.13.0-dev\nExample:\n    &gt; spacesavers2_pdq_create_db -f /path/to/sqlitedbfile\n</code></pre>"},{"location":"pdq_create_db/#output","title":"Output","text":""},{"location":"pdq_create_db/#db-file","title":"db file","text":"<p>sqlite \".db\" file with 4 tables</p> <pre><code>% sqlite3 pdq.db\nSQLite version 3.26.0 2018-12-01 12:34:55\nEnter \".help\" for usage hints.\nsqlite&gt; .table\ndatamounts  datapoints  dates       users\nsqlite&gt; .schema\nCREATE TABLE users (\n                        user_id INTEGER PRIMARY KEY,\n                        username TEXT NOT NULL,\n                        first_name TEXT NOT NULL,\n                        last_name TEXT NOT NULL\n                    );\nCREATE TABLE dates (\n                        date_int INTEGER PRIMARY KEY,\n                        date_text TEXT UNIQUE NOT NULL\n                    );\nCREATE TABLE datamounts (\n                        datamount_id INTEGER PRIMARY KEY,\n                        datamount_name TEXT UNIQUE NOT NULL\n                    );\nCREATE TABLE datapoints (\n                        datapoint_id INTEGER PRIMARY KEY,\n                        date_int INTEGER,\n                        datamount_id INTEGER,\n                        user_id INTEGER,\n                        ninodes INTEGER,\n                        nbytes INTEGER,\n                        FOREIGN KEY (user_id) REFERENCES users(user_id),\n                        FOREIGN KEY (datamount_id) REFERENCES datamounts(datamount_id),\n                        FOREIGN KEY (date_int) REFERENCES dates(date_int)\n                    );\n</code></pre>"},{"location":"pdq_update_db/","title":"pdq_update_db","text":""},{"location":"pdq_update_db/#spacesavers2_pdq_update_db","title":"spacesavers2_pdq_update_db","text":"<p>pdq = Pretty Darn Quick</p> <p><code>spacesavers2_pdq</code> creates TSV (or JSON) file per-datamount per-run (typically per-date). If run daily, this soon creates a lot of files to keep track of. Hence, it is best to save the data in a sqlite db. <code>spacesavers2_pdq_create_db</code> command creates the basic schema for that db. Then this command can be used to populate the database.</p> <p></p>"},{"location":"pdq_update_db/#inputs","title":"Inputs","text":"<ul> <li><code>--tsv</code>: <code>.tsv</code> or <code>.tsv.gz</code> created using <code>spacesavers2_pdq</code></li> <li><code>--database</code>: <code>.db</code> file created using <code>spacesavers2_pdq_create_db</code></li> <li><code>--datamount</code>: eg. <code>CCBR</code> or <code>CCBR_Pipeliner</code></li> <li><code>--date</code>: integer date in YYYYMMDD format</li> </ul> <pre><code>usage: spacesavers2_pdq_update_db [-h] -t TSV -o DATABASE -m DATAMOUNT -d DATE [-v]\n\nspacesavers2_pdq_create_db: update/append date from TSV to DB file\n\noptions:\n  -h, --help            show this help message and exit\n  -t TSV, --tsv TSV     spacesavers2_pdq output TSV file\n  -o DATABASE, --database DATABASE\n                        database file path (use spacesavers2_pdb_create_db to create if it does not exists.)\n  -m DATAMOUNT, --datamount DATAMOUNT\n                        name of the datamount eg. CCBR or CCBR_Pipeliner\n  -d DATE, --date DATE  date in YYYYMMDD integer format\n  -v, --version         show program's version number and exit\n\nVersion:\n    v0.13.0-dev\nExample:\n    &gt; spacesavers2_pdq_update_db -t /path/to/tsv -o /path/to/db -m datamount_name -d date\n</code></pre>"},{"location":"pdq_update_db/#output","title":"Output","text":""},{"location":"pdq_update_db/#updated-db-file","title":"updated db file","text":"<p>sqlite \".db\" file with 4 tables is updated.</p> <p>NOTE:</p> <ul> <li>new users are automatically added to \"users\" table </li> <li>new datemounts are automatically added to \"datamounts\" table</li> <li>new dates are automatically added to \"dates\" table</li> <li>if &gt;0 datapoints exist in the \".db\" for a (date + datamount) combination then warning is displayed and no data is appended</li> </ul>"},{"location":"usurp/","title":"usurp","text":""},{"location":"usurp/#spacesavers2_usurp","title":"spacesavers2_usurp","text":"<p>This takes in the TSV file generated by <code>spacesavers2_grubbers</code> and a hash from its first column to:</p> <ul> <li>find the row corresponding to the hash</li> <li>keep one copy as of the duplicates as \"original copy\"</li> <li>delete other copies and replace them with hard (or soft) links</li> </ul> <p>Deleting these high-value duplicates has the biggest impact on the users overall digital footprint.</p>"},{"location":"usurp/#inputs","title":"Inputs","text":"<ul> <li><code>--grubber</code> output file from <code>spacesavers2_grubbers</code>.</li> <li><code>--hash</code> a hash from its first column of the grubber TSV.</li> <li><code>--force</code> (OPTIONAL) if the duplicates are cross-device then hard links cannot be made, with <code>--force</code> you can force using sym-links instead.</li> </ul> <p>The GRUBBER file has the following columns: | Column | Description                           | | ------ | ------------------------------------- | | 1      | combined hash                         | | 2      | number of duplicates found            | | 3      | total size of all duplicates (human readable)          | | 4      | size of each duplicate (human readable)               | | 5      | original file      | | 6      | \";\"-separated list of duplicates files                       |</p> <pre><code>usage: spacesavers2_usurp [-h] -g GRUBBER -x HASH [-f | --force | --no-force]\n\nspacesavers2_usurp: delete all but one copy of the file matching the hash and replace all other copies with hardlinks\n\noptions:\n  -h, --help            show this help message and exit\n  -g GRUBBER, --grubber GRUBBER\n                        spacesavers2_grubbers output TSV file\n  -x HASH, --hash HASH  hash (or unique partial hash) from column 1 of spacesavers2_grubbers TSV file\n  -f, --force, --no-force\n                        forcefully create symlink if hardlink is not possible\n\nVersion:\n    v0.8\nExample:\n    &gt; spacesavers2_usurp -g grubbers.TSV -h someHash\n</code></pre>"},{"location":"usurp/#outputs","title":"Outputs","text":"<p>On screen confirmation that the duplicates are replaced with hard (or soft) links.</p>"}]}