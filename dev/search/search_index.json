{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":":rocket: spacesavers2 :rocket: \u00b6 Background \u00b6 spacesavers2 crawls through the provided folder (and its subfolders), gathers stats for each file like size, inode, user/group information, etc., calculates unique hashes for each file, using the information gathers determines \"duplicates\", reports \"high-value\" duplicates, i.e., the ones that will give back most diskspace, if deleted,and makes a \"counts-matrix\" style matrix with folders as rownames and users a columnnames with each cell representing duplicate bytes. New improved parallel implementation of spacesavers . spacesavers is soon to be decommissioned! Note: spacesavers2 requires python version 3.11 or later and the xxhash library. These dependencies are already installed on biowulf (as a conda env). The environment for running spacesavers2 can get set up using the shared conda environment on biowulf or frce. on biowulf: . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" && \\ conda activate /data/CCBR_Pipeliner/db/PipeDB/Conda/envs/py311 on frce: . \"/mnt/projects/CCBR-Pipelines/resources/miniconda3/etc/profile.d/conda.sh\" && \\ conda activate /mnt/projects/CCBR-Pipelines/resources/miniconda3/envs/py311 Commands \u00b6 spacesavers2 has the following Basic commands: spacesavers2_catalog spacesavers2_mimeo spacesavers2_grubbers spacesavers2_usurp spacesavers2_e2e spacesavers2_pdq Use case \u00b6 One would like to monitor the per-user digital footprint on shared data drives like /data/CCBR on biowulf. Setting the spacesavers2_e2e as a weekly cronjob will allow automation of this task. slurm_job script is also provided to work as a template for using the job scheduler on the HPC to submit (possibly, as cronjob).","title":"Background"},{"location":"#rocket-spacesavers2-rocket","text":"","title":":rocket: spacesavers2 :rocket:"},{"location":"#background","text":"spacesavers2 crawls through the provided folder (and its subfolders), gathers stats for each file like size, inode, user/group information, etc., calculates unique hashes for each file, using the information gathers determines \"duplicates\", reports \"high-value\" duplicates, i.e., the ones that will give back most diskspace, if deleted,and makes a \"counts-matrix\" style matrix with folders as rownames and users a columnnames with each cell representing duplicate bytes. New improved parallel implementation of spacesavers . spacesavers is soon to be decommissioned! Note: spacesavers2 requires python version 3.11 or later and the xxhash library. These dependencies are already installed on biowulf (as a conda env). The environment for running spacesavers2 can get set up using the shared conda environment on biowulf or frce. on biowulf: . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" && \\ conda activate /data/CCBR_Pipeliner/db/PipeDB/Conda/envs/py311 on frce: . \"/mnt/projects/CCBR-Pipelines/resources/miniconda3/etc/profile.d/conda.sh\" && \\ conda activate /mnt/projects/CCBR-Pipelines/resources/miniconda3/envs/py311","title":"Background"},{"location":"#commands","text":"spacesavers2 has the following Basic commands: spacesavers2_catalog spacesavers2_mimeo spacesavers2_grubbers spacesavers2_usurp spacesavers2_e2e spacesavers2_pdq","title":"Commands"},{"location":"#use-case","text":"One would like to monitor the per-user digital footprint on shared data drives like /data/CCBR on biowulf. Setting the spacesavers2_e2e as a weekly cronjob will allow automation of this task. slurm_job script is also provided to work as a template for using the job scheduler on the HPC to submit (possibly, as cronjob).","title":"Use case"},{"location":"catalog/","text":"spacesavers2_catalog \u00b6 This uses glob library to list all files in a user-provided folder recursively. For each file it gathers information like: - filesize - check if it is symlink - inode - creation and modificatino time - user id - group id For each file it also computes a unique hash (using xxHash library) for: - top chunk of the file - bottom chunk of the file ( -e option) Inputs \u00b6 --folder : Path to the folder to run spacesavers2_catalog on. --threads : spacesavers2_catalog uses multiprocessing library to parallelize orchestration. This defines the number of threads to run in parallel. --buffersize : This defines the size of the top (or bottom) chunk of the file to be used to by xxHash. (default 128 KB) --ignoreheadersize : This defines the size of the top of the file to be ignored before reading the top chunk for xxHash calculation. This is useful for ignore the header portion of files like BAM or BIGWIG which may have the same top chunk (Eg. samples aligned to the same reference index will generate BAMs with the same header hence original spacesaver , which only looks at the top chunk, may call them duplicates. spacesavers2 tries to do a better job at this.). (default 1 MB) --se : Comma-separated special extensions for home spacesavers2 ignores the headers before extracting the top chunk for xxHash calculation. The default is \"bam,bai,bigwig,bw,csi\". --bottomhash : Default False. Use the bottom chunk along with the top chunk of the file for xxHash calculation. --outfile : If not supplied then the optput is written to the screen. --brokenlink : Default False. Create a file listing broken symlinks per-user. --geezer : Default False. Create a file listing really old files per-user. Output files have 3 columns: age, size, path --geezerage : Default 5 * 365. age in days to be considered a geezer file. --geezersize : Default 10 MiB. minimum size in bytes of geezer file to be reported. NOTE: spacesavers2_catalog reports errors (eg. cannot read file) to STDERR usage: spacesavers2_catalog [ -h ] -f FOLDER [ -p THREADS ] [ -b BUFFERSIZE ] [ -i IGNOREHEADERSIZE ] [ -x SE ] [ -s ST_BLOCK_BYTE_SIZE ] [ -o OUTFILE ] [ -e | --bottomhash | --no-bottomhash ] [ -l | --brokenlink | --no-brokenlink ] [ -g | --geezers | --no-geezers ] [ -a GEEZERAGE ] [ -z GEEZERSIZE ] [ -v ] spacesavers2_catalog: get per file info. options: -h, --help show this help message and exit -f FOLDER, --folder FOLDER spacesavers2_catalog will be run on all files in this folder and its subfolders -p THREADS, --threads THREADS number of threads to be used ( default 4 ) -b BUFFERSIZE, --buffersize BUFFERSIZE buffersize for xhash creation ( default = 128 * 1028 bytes ) -i IGNOREHEADERSIZE, --ignoreheadersize IGNOREHEADERSIZE this sized header of the file is ignored before extracting buffer of buffersize for xhash creation ( only for special extensions files ) default = 1024 * 1024 * 1024 bytes -x SE, --se SE comma separated list of special extensions ( default = bam,bai,bigwig,bw,csi ) -s ST_BLOCK_BYTE_SIZE, --st_block_byte_size ST_BLOCK_BYTE_SIZE st_block_byte_size on current filesystem ( default 512 ) -o OUTFILE, --outfile OUTFILE outfile ... catalog file .. by default output is printed to screen -e, --bottomhash, --no-bottomhash separately calculated second hash for the bottom/end of the file. -l, --brokenlink, --no-brokenlink output per-user broken links list. -g, --geezers, --no-geezers output per-user geezer files list. -a GEEZERAGE, --geezerage GEEZERAGE age in days to be considered a geezer file ( default 5yrs ... 5 * 365 ) . -z GEEZERSIZE, --geezersize GEEZERSIZE minimum size in bytes of geezer file to be reported ( default 10MiB ... 10 * 1024 * 1024 ) . -v, --version show program ' s version number and exit Version: v0.11.4 Example: > spacesavers2_catalog -f /path/to/folder -p 56 -e -l -g Output \u00b6 catalog file \u00b6 spacesavers2_catalog creates one semi-colon seperated output line per input file. Here is an example line: % head -n1 test.catalog \"/data/CBLCCBR/kopardevn_tmp/spacesavers2_testing/_data_CCBR_Pipeliner_db_PipeDB_Indices.ls.old\" ; False ; 1653453 ; 47 ; 372851499 ; 1 ; 1 ; 5 ; 5 ; 37513 ; 57886 ; 4707e661a1f3beca1861b9e0e0177461 ; 52e5038016c3dce5b6cdab635765cc79 ; The 13 items in the line are as follows: Column Description Example 1 File absolute path \"/data/CBLCCBR/kopardevn_tmp/spacesavers2_testing/_data_CCBR_Pipeliner_db_PipeDB_Indices.ls.old\" 2 Is file a symlink? FALSE 3 file size in bytes 1653453 4 file device 47 5 file inode 372851499 6 number of hardlinks 1 7 access age in days 1 8 modification age in days 5 9 creation age in days 5 10 user id 37513 11 group id 57886 12 top chunk xxHash 4707e661a1f3beca1861b9e0e0177461 13 bottom chunk xxHash 52e5038016c3dce5b6cdab635765cc79 NOTE: Some files may have \";\" or spaces in their name, hence adding quotes around the absolute file path. broken links file \u00b6 simply lists the paths which are symbolic links, but the destination files do not exist anymore! one file per username. DISCLAIMER: - may contain false-positives if the user running spacesavers2_catalog does not have read access to the symlinks destination geezer file \u00b6 lists really old files (default > 5 years). list has 3 columns: age, size and path. one file per username.","title":"catalog"},{"location":"catalog/#spacesavers2_catalog","text":"This uses glob library to list all files in a user-provided folder recursively. For each file it gathers information like: - filesize - check if it is symlink - inode - creation and modificatino time - user id - group id For each file it also computes a unique hash (using xxHash library) for: - top chunk of the file - bottom chunk of the file ( -e option)","title":"spacesavers2_catalog"},{"location":"catalog/#inputs","text":"--folder : Path to the folder to run spacesavers2_catalog on. --threads : spacesavers2_catalog uses multiprocessing library to parallelize orchestration. This defines the number of threads to run in parallel. --buffersize : This defines the size of the top (or bottom) chunk of the file to be used to by xxHash. (default 128 KB) --ignoreheadersize : This defines the size of the top of the file to be ignored before reading the top chunk for xxHash calculation. This is useful for ignore the header portion of files like BAM or BIGWIG which may have the same top chunk (Eg. samples aligned to the same reference index will generate BAMs with the same header hence original spacesaver , which only looks at the top chunk, may call them duplicates. spacesavers2 tries to do a better job at this.). (default 1 MB) --se : Comma-separated special extensions for home spacesavers2 ignores the headers before extracting the top chunk for xxHash calculation. The default is \"bam,bai,bigwig,bw,csi\". --bottomhash : Default False. Use the bottom chunk along with the top chunk of the file for xxHash calculation. --outfile : If not supplied then the optput is written to the screen. --brokenlink : Default False. Create a file listing broken symlinks per-user. --geezer : Default False. Create a file listing really old files per-user. Output files have 3 columns: age, size, path --geezerage : Default 5 * 365. age in days to be considered a geezer file. --geezersize : Default 10 MiB. minimum size in bytes of geezer file to be reported. NOTE: spacesavers2_catalog reports errors (eg. cannot read file) to STDERR usage: spacesavers2_catalog [ -h ] -f FOLDER [ -p THREADS ] [ -b BUFFERSIZE ] [ -i IGNOREHEADERSIZE ] [ -x SE ] [ -s ST_BLOCK_BYTE_SIZE ] [ -o OUTFILE ] [ -e | --bottomhash | --no-bottomhash ] [ -l | --brokenlink | --no-brokenlink ] [ -g | --geezers | --no-geezers ] [ -a GEEZERAGE ] [ -z GEEZERSIZE ] [ -v ] spacesavers2_catalog: get per file info. options: -h, --help show this help message and exit -f FOLDER, --folder FOLDER spacesavers2_catalog will be run on all files in this folder and its subfolders -p THREADS, --threads THREADS number of threads to be used ( default 4 ) -b BUFFERSIZE, --buffersize BUFFERSIZE buffersize for xhash creation ( default = 128 * 1028 bytes ) -i IGNOREHEADERSIZE, --ignoreheadersize IGNOREHEADERSIZE this sized header of the file is ignored before extracting buffer of buffersize for xhash creation ( only for special extensions files ) default = 1024 * 1024 * 1024 bytes -x SE, --se SE comma separated list of special extensions ( default = bam,bai,bigwig,bw,csi ) -s ST_BLOCK_BYTE_SIZE, --st_block_byte_size ST_BLOCK_BYTE_SIZE st_block_byte_size on current filesystem ( default 512 ) -o OUTFILE, --outfile OUTFILE outfile ... catalog file .. by default output is printed to screen -e, --bottomhash, --no-bottomhash separately calculated second hash for the bottom/end of the file. -l, --brokenlink, --no-brokenlink output per-user broken links list. -g, --geezers, --no-geezers output per-user geezer files list. -a GEEZERAGE, --geezerage GEEZERAGE age in days to be considered a geezer file ( default 5yrs ... 5 * 365 ) . -z GEEZERSIZE, --geezersize GEEZERSIZE minimum size in bytes of geezer file to be reported ( default 10MiB ... 10 * 1024 * 1024 ) . -v, --version show program ' s version number and exit Version: v0.11.4 Example: > spacesavers2_catalog -f /path/to/folder -p 56 -e -l -g","title":"Inputs"},{"location":"catalog/#output","text":"","title":"Output"},{"location":"catalog/#catalog-file","text":"spacesavers2_catalog creates one semi-colon seperated output line per input file. Here is an example line: % head -n1 test.catalog \"/data/CBLCCBR/kopardevn_tmp/spacesavers2_testing/_data_CCBR_Pipeliner_db_PipeDB_Indices.ls.old\" ; False ; 1653453 ; 47 ; 372851499 ; 1 ; 1 ; 5 ; 5 ; 37513 ; 57886 ; 4707e661a1f3beca1861b9e0e0177461 ; 52e5038016c3dce5b6cdab635765cc79 ; The 13 items in the line are as follows: Column Description Example 1 File absolute path \"/data/CBLCCBR/kopardevn_tmp/spacesavers2_testing/_data_CCBR_Pipeliner_db_PipeDB_Indices.ls.old\" 2 Is file a symlink? FALSE 3 file size in bytes 1653453 4 file device 47 5 file inode 372851499 6 number of hardlinks 1 7 access age in days 1 8 modification age in days 5 9 creation age in days 5 10 user id 37513 11 group id 57886 12 top chunk xxHash 4707e661a1f3beca1861b9e0e0177461 13 bottom chunk xxHash 52e5038016c3dce5b6cdab635765cc79 NOTE: Some files may have \";\" or spaces in their name, hence adding quotes around the absolute file path.","title":"catalog file"},{"location":"catalog/#broken-links-file","text":"simply lists the paths which are symbolic links, but the destination files do not exist anymore! one file per username. DISCLAIMER: - may contain false-positives if the user running spacesavers2_catalog does not have read access to the symlinks destination","title":"broken links file"},{"location":"catalog/#geezer-file","text":"lists really old files (default > 5 years). list has 3 columns: age, size and path. one file per username.","title":"geezer file"},{"location":"e2e/","text":"spacesavers2_e2e \u00b6 This is a wrapper to run all the spacesavers2 commands in the correct order. It automatically: adds appropriate prefixes to output files (including time) catalogs files in the folder provided finds duplicates finds high-value duplicates creates blamematrix This is ideal wrapper to be added as a cronjob. % spacesavers2_e2e --help usage: spacesavers2_e2e [ -h ] -i INFOLDER [ -p THREADS ] [ -q QUOTA ] -o OUTFOLDER End-to-end run of spacesavers2 options: -h, --help show this help message and exit -f FOLDER, --folder FOLDER Folder to run spacesavers_catalog on. -p THREADS, --threads THREADS number of threads to use -d MAXDEPTH, --maxdepth MAXDEPTH maxdepth for mimeo -l LIMIT, --limit LIMIT limit for running spacesavers_grubbers -q QUOTA, --quota QUOTA total size of the volume ( default = 200 for /data/CCBR ) -o OUTFOLDER, --outfolder OUTFOLDER Folder where all spacesavers_e2e output files will be saved","title":"e2e"},{"location":"e2e/#spacesavers2_e2e","text":"This is a wrapper to run all the spacesavers2 commands in the correct order. It automatically: adds appropriate prefixes to output files (including time) catalogs files in the folder provided finds duplicates finds high-value duplicates creates blamematrix This is ideal wrapper to be added as a cronjob. % spacesavers2_e2e --help usage: spacesavers2_e2e [ -h ] -i INFOLDER [ -p THREADS ] [ -q QUOTA ] -o OUTFOLDER End-to-end run of spacesavers2 options: -h, --help show this help message and exit -f FOLDER, --folder FOLDER Folder to run spacesavers_catalog on. -p THREADS, --threads THREADS number of threads to use -d MAXDEPTH, --maxdepth MAXDEPTH maxdepth for mimeo -l LIMIT, --limit LIMIT limit for running spacesavers_grubbers -q QUOTA, --quota QUOTA total size of the volume ( default = 200 for /data/CCBR ) -o OUTFOLDER, --outfolder OUTFOLDER Folder where all spacesavers_e2e output files will be saved","title":"spacesavers2_e2e"},{"location":"grubbers/","text":"spacesavers2_grubbers \u00b6 This takes in the mimeo.files.gz generated by spacesavers2_mimeo and processes it to: sort duplicates by total size reports the \"high-value\" duplicates. Deleting these high-value duplicates first will have the biggest impact on the users overall digital footprint. Inputs \u00b6 --filesgz output file from spacesavers2_mimeo . --limit lower cut-off for output display (default 5 GiB). This means that duplicates with overall size of less than 5 GiB will not be displayed. Set 0 to report all. \u2570\u2500\u25cb spacesavers2_grubbers --help spacesavers2_grubbers:00000.00s:version: v0.10.2-dev usage: spacesavers2_grubbers [ -h ] -f FILESGZ [ -l LIMIT ] [ -o OUTFILE ] [ -v ] spacesavers2_grubbers: get list of large duplicates sorted by total size options: -h, --help show this help message and exit -f FILESGZ, --filesgz FILESGZ spacesavers2_mimeo prefix.<user>.mimeo.files.gz file -l LIMIT, --limit LIMIT stop showing duplicates with total size smaller than ( 5 default ) GiB. Set 0 for unlimited. -o OUTFILE, --outfile OUTFILE output tab-delimited file ( default STDOUT ) -v, --version show program ' s version number and exit Version: v0.10.2-dev Example: > spacesavers2_grubbers -f /output/from/spacesavers2_finddup/prefix.files.gz Outputs \u00b6 The output is displayed on STDOUT and is tab-delimited with these columns: Column Description 1 combined hash 2 number of duplicates found 3 total size of all duplicates (human readable) 4 size of each duplicate (human readable) 5 original file 6 \";\"-separated list of duplicates files Here is an example output line: 183e9dc341073d9b75c817f5ed07b9ac#183e9dc341073d9b75c817f5ed07b9ac 5 0 .07 KiB 0 .01 KiB \"/data/CCBR/abdelmaksoudaa/test/a\" \"/data/CCBR/abdelmaksoudaa/test/b\" ; \"/data/CCBR/abde lmaksoudaa/test/c\" ; \"/data/CCBR/abdelmaksoudaa/test/d\" ; \"/data/CCBR/abdelmaksoudaa/test/e\" ; \"/data/CCBR/abdelmaksoudaa/test/f\" spacesavers2_grubbers is typical used to find the \"low-hanging\" fruits ... aka ... the \"high-value\" duplicates which need to be deleted first to quickly have the biggest impact on the users overall digital footprint.","title":"grubbers"},{"location":"grubbers/#spacesavers2_grubbers","text":"This takes in the mimeo.files.gz generated by spacesavers2_mimeo and processes it to: sort duplicates by total size reports the \"high-value\" duplicates. Deleting these high-value duplicates first will have the biggest impact on the users overall digital footprint.","title":"spacesavers2_grubbers"},{"location":"grubbers/#inputs","text":"--filesgz output file from spacesavers2_mimeo . --limit lower cut-off for output display (default 5 GiB). This means that duplicates with overall size of less than 5 GiB will not be displayed. Set 0 to report all. \u2570\u2500\u25cb spacesavers2_grubbers --help spacesavers2_grubbers:00000.00s:version: v0.10.2-dev usage: spacesavers2_grubbers [ -h ] -f FILESGZ [ -l LIMIT ] [ -o OUTFILE ] [ -v ] spacesavers2_grubbers: get list of large duplicates sorted by total size options: -h, --help show this help message and exit -f FILESGZ, --filesgz FILESGZ spacesavers2_mimeo prefix.<user>.mimeo.files.gz file -l LIMIT, --limit LIMIT stop showing duplicates with total size smaller than ( 5 default ) GiB. Set 0 for unlimited. -o OUTFILE, --outfile OUTFILE output tab-delimited file ( default STDOUT ) -v, --version show program ' s version number and exit Version: v0.10.2-dev Example: > spacesavers2_grubbers -f /output/from/spacesavers2_finddup/prefix.files.gz","title":"Inputs"},{"location":"grubbers/#outputs","text":"The output is displayed on STDOUT and is tab-delimited with these columns: Column Description 1 combined hash 2 number of duplicates found 3 total size of all duplicates (human readable) 4 size of each duplicate (human readable) 5 original file 6 \";\"-separated list of duplicates files Here is an example output line: 183e9dc341073d9b75c817f5ed07b9ac#183e9dc341073d9b75c817f5ed07b9ac 5 0 .07 KiB 0 .01 KiB \"/data/CCBR/abdelmaksoudaa/test/a\" \"/data/CCBR/abdelmaksoudaa/test/b\" ; \"/data/CCBR/abde lmaksoudaa/test/c\" ; \"/data/CCBR/abdelmaksoudaa/test/d\" ; \"/data/CCBR/abdelmaksoudaa/test/e\" ; \"/data/CCBR/abdelmaksoudaa/test/f\" spacesavers2_grubbers is typical used to find the \"low-hanging\" fruits ... aka ... the \"high-value\" duplicates which need to be deleted first to quickly have the biggest impact on the users overall digital footprint.","title":"Outputs"},{"location":"mimeo/","text":"spacesavers2_mimeo \u00b6 This takes in the catalog file generated by spacesavers2_catalog and processes it to: find duplicates create per-user summary reports for each user (and all users). Inputs \u00b6 --catalog is the output file from spacesavers2_catalog . Thus, spacesavers2_catalog needs to be run before running spacesavers2_mimeo . --maxdepth maximum folder depth upto which reports are aggregated --outdir path to the output folder --prefix prefix to be added to the output file names eg. date etc. --duplicatesonly only report duplicates in the .files.gz output files. This saves a lot of disc space. (Highly recommended!) --quota defines the size of the overall file mount. (eg. 200 TB for /data/CCBR on BIOWULF.) OccScore is dependent on this and should be provided appropriately for accurate results. % spacesavers2_mimeo --help usage: spacesavers2_mimeo [ -h ] -f CATALOG [ -d MAXDEPTH ] [ -o OUTDIR ] [ -p PREFIX ] [ -q QUOTA ] [ -z | --duplicatesonly | --no-duplicatesonly ] [ -k | --kronaplot | --no-kronaplot ] [ -v ] spacesavers2_mimeo: find duplicates options: -h, --help show this help message and exit -f CATALOG, --catalog CATALOG spacesavers2_catalog output from STDIN or from catalog file -d MAXDEPTH, --maxdepth MAXDEPTH folder max. depth upto which reports are aggregated ... absolute path is used to calculate depth ( Default: 10 ) -o OUTDIR, --outdir OUTDIR output folder -p PREFIX, --prefix PREFIX prefix for all output files -q QUOTA, --quota QUOTA total quota of the mount eg. 200 TB for /data/CCBR -z, --duplicatesonly, --no-duplicatesonly Print only duplicates to per user output file. -k, --kronaplot, --no-kronaplot Make kronaplots for duplicates. ( ktImportText must be in PATH! ) -v, --version show program ' s version number and exit Version: v0.10.2-dev Example: > spacesavers2_mimeo -f /output/from/spacesavers2_catalog -o /path/to/output/folder -d 7 -q 10 -k Outputs \u00b6 After completion of run, spacesavers2_mimeo creates *.mimeo.files.gz (list of files per user + one \"allusers\" file) and .summary.txt (overall stats at various depths) files in the provided output folder. if -k is provided (and ktImportText from kronatools is in PATH) then krona specific TSV and HTML pages are also generated. It also generates a blamematrix.tsv file with folders on rows and users on columns with duplicate bytes per-folder-per-user. This file can be used to create a \"heatmap\" to pinpoint folder with highest duplicates overall as well as on a per-user basis. Here are the details: Duplicates \u00b6 spacesavers2_mimeo uses the following logic to find duplicates: Bin files by their top (and bottom) xxHashes irrespective of user id (allusers mode) Check if each bin has unique sized files. If a bin has more than 1 size, then it needs to be binned further. Sometimes, xxHash of top and bottom chunks also gives the same combination of hash for differing files. These files will have different sizes. Hence, re-bin them accordingly. If same size, then check inodes. If all files in the same bin have the same inode, then these are just hard-links. But, if there are multiple inodes, then we have duplicates ! If we have duplicates, then spacesavers2_mimeo keeps track of number of duplicates per bin. Number of duplicates is equal to number of inodes in each bin minus one. If we have duplicates, then the oldest file is identified and considered to be the original file. All other files are marked duplicate , irrespective of user id. duplicate files are reported in gzip format with the following columns for all users and per-user basis Here is what the .files.gz file columns (space-separated) represent: Column Description 1 top chunk and bottom chunk hashes separated by \"#\" 2 separator \":\" 3 Number of duplicates files (not duplicate inodes) 4 Size of each file 5 List of users duplicates serapated by \"##\" NOTE: Number of dupicate files can be greater than number of duplicate inodes as each file can have multiple hard links already. Hence, while calculating total duplicate bytes we use (total_number_of_unique_inodes_per_group_of_duplicate_files - 1) X size_of_each_file. The \"minus 1\" is to not count the size of the original file. Each file in the last column above is \";\" separated with the same 13 items as described in the catalog file. The only difference is that the username and groupame are now appended to each file entry. Along with creating one .mimeo.files.gz and .mimeo.summary.txt file per user encountered, spacesavers2_mimeo also generates a allusers.mimeo.files.gz file for all users combined. This file is later used by spacesavers2_blamematrix as input. Summaries \u00b6 Summaries, files ending with .mimeo.summary.txt are collected and reported for all users ( allusers.mimeo.summary.txt ) and per-user ( USERNAME.mimeo.summary.txt ) basis for user-defined depth (and beyond). The columns (tab-delimited) in the summary file: Column Description 1 absolute path 2 total bytes 3 duplicate bytes 4 percent duplicate bytes 5 total files 6 duplicate files 7 percent duplicate files 8 average file age of all files (days) 9 average file age of duplicates (days) 10 AgeScore 11 DupScore 12 OccScore 13 OverallScore For columns 10 through 13, the same logic is used as spacesavers . KronaTSV and KronaHTML \u00b6 KronaTSV is tab-delimited with first column showing the number of duplicate bytes and every subsequent column giving the folder depths. ktImportText is then used to convert the KronaTSV to KronaHTML which can be shared easily and only needs a HTML5 supporting browser for viewing. Blamematrix \u00b6 rows are folders as 1 level deeper than the \"mindepth\" columns are all individual usernames, plus an \"allusers\" column only duplicate-bytes are reported","title":"mimeo"},{"location":"mimeo/#spacesavers2_mimeo","text":"This takes in the catalog file generated by spacesavers2_catalog and processes it to: find duplicates create per-user summary reports for each user (and all users).","title":"spacesavers2_mimeo"},{"location":"mimeo/#inputs","text":"--catalog is the output file from spacesavers2_catalog . Thus, spacesavers2_catalog needs to be run before running spacesavers2_mimeo . --maxdepth maximum folder depth upto which reports are aggregated --outdir path to the output folder --prefix prefix to be added to the output file names eg. date etc. --duplicatesonly only report duplicates in the .files.gz output files. This saves a lot of disc space. (Highly recommended!) --quota defines the size of the overall file mount. (eg. 200 TB for /data/CCBR on BIOWULF.) OccScore is dependent on this and should be provided appropriately for accurate results. % spacesavers2_mimeo --help usage: spacesavers2_mimeo [ -h ] -f CATALOG [ -d MAXDEPTH ] [ -o OUTDIR ] [ -p PREFIX ] [ -q QUOTA ] [ -z | --duplicatesonly | --no-duplicatesonly ] [ -k | --kronaplot | --no-kronaplot ] [ -v ] spacesavers2_mimeo: find duplicates options: -h, --help show this help message and exit -f CATALOG, --catalog CATALOG spacesavers2_catalog output from STDIN or from catalog file -d MAXDEPTH, --maxdepth MAXDEPTH folder max. depth upto which reports are aggregated ... absolute path is used to calculate depth ( Default: 10 ) -o OUTDIR, --outdir OUTDIR output folder -p PREFIX, --prefix PREFIX prefix for all output files -q QUOTA, --quota QUOTA total quota of the mount eg. 200 TB for /data/CCBR -z, --duplicatesonly, --no-duplicatesonly Print only duplicates to per user output file. -k, --kronaplot, --no-kronaplot Make kronaplots for duplicates. ( ktImportText must be in PATH! ) -v, --version show program ' s version number and exit Version: v0.10.2-dev Example: > spacesavers2_mimeo -f /output/from/spacesavers2_catalog -o /path/to/output/folder -d 7 -q 10 -k","title":"Inputs"},{"location":"mimeo/#outputs","text":"After completion of run, spacesavers2_mimeo creates *.mimeo.files.gz (list of files per user + one \"allusers\" file) and .summary.txt (overall stats at various depths) files in the provided output folder. if -k is provided (and ktImportText from kronatools is in PATH) then krona specific TSV and HTML pages are also generated. It also generates a blamematrix.tsv file with folders on rows and users on columns with duplicate bytes per-folder-per-user. This file can be used to create a \"heatmap\" to pinpoint folder with highest duplicates overall as well as on a per-user basis. Here are the details:","title":"Outputs"},{"location":"mimeo/#duplicates","text":"spacesavers2_mimeo uses the following logic to find duplicates: Bin files by their top (and bottom) xxHashes irrespective of user id (allusers mode) Check if each bin has unique sized files. If a bin has more than 1 size, then it needs to be binned further. Sometimes, xxHash of top and bottom chunks also gives the same combination of hash for differing files. These files will have different sizes. Hence, re-bin them accordingly. If same size, then check inodes. If all files in the same bin have the same inode, then these are just hard-links. But, if there are multiple inodes, then we have duplicates ! If we have duplicates, then spacesavers2_mimeo keeps track of number of duplicates per bin. Number of duplicates is equal to number of inodes in each bin minus one. If we have duplicates, then the oldest file is identified and considered to be the original file. All other files are marked duplicate , irrespective of user id. duplicate files are reported in gzip format with the following columns for all users and per-user basis Here is what the .files.gz file columns (space-separated) represent: Column Description 1 top chunk and bottom chunk hashes separated by \"#\" 2 separator \":\" 3 Number of duplicates files (not duplicate inodes) 4 Size of each file 5 List of users duplicates serapated by \"##\" NOTE: Number of dupicate files can be greater than number of duplicate inodes as each file can have multiple hard links already. Hence, while calculating total duplicate bytes we use (total_number_of_unique_inodes_per_group_of_duplicate_files - 1) X size_of_each_file. The \"minus 1\" is to not count the size of the original file. Each file in the last column above is \";\" separated with the same 13 items as described in the catalog file. The only difference is that the username and groupame are now appended to each file entry. Along with creating one .mimeo.files.gz and .mimeo.summary.txt file per user encountered, spacesavers2_mimeo also generates a allusers.mimeo.files.gz file for all users combined. This file is later used by spacesavers2_blamematrix as input.","title":"Duplicates"},{"location":"mimeo/#summaries","text":"Summaries, files ending with .mimeo.summary.txt are collected and reported for all users ( allusers.mimeo.summary.txt ) and per-user ( USERNAME.mimeo.summary.txt ) basis for user-defined depth (and beyond). The columns (tab-delimited) in the summary file: Column Description 1 absolute path 2 total bytes 3 duplicate bytes 4 percent duplicate bytes 5 total files 6 duplicate files 7 percent duplicate files 8 average file age of all files (days) 9 average file age of duplicates (days) 10 AgeScore 11 DupScore 12 OccScore 13 OverallScore For columns 10 through 13, the same logic is used as spacesavers .","title":"Summaries"},{"location":"mimeo/#kronatsv-and-kronahtml","text":"KronaTSV is tab-delimited with first column showing the number of duplicate bytes and every subsequent column giving the folder depths. ktImportText is then used to convert the KronaTSV to KronaHTML which can be shared easily and only needs a HTML5 supporting browser for viewing.","title":"KronaTSV and KronaHTML"},{"location":"mimeo/#blamematrix","text":"rows are folders as 1 level deeper than the \"mindepth\" columns are all individual usernames, plus an \"allusers\" column only duplicate-bytes are reported","title":"Blamematrix"},{"location":"pdq/","text":"spacesavers2_pdq \u00b6 pdq = Pretty Darn Quick This uses glob library to list all files in a user-provided folder recursively. For each user it gathers information like: - total number of inodes - total number of bytes It is quick tool to gather datapoints to monitor filesystem usage. Typically, can be run once daily and compared with previous days run to find large changes. Inputs \u00b6 --folder : Path to the folder to run spacesavers2_pdq on. --threads : spacesavers2_pdq uses multiprocessing library to parallelize orchestration. This defines the number of threads to run in parallel. --outfile : If not supplied then the output is written to the screen. --json : Optional, if provided output is also written in JSON format. NOTE: spacesavers2_pdq reports errors (eg. cannot read file) to STDERR usage: spacesavers2_pdq [ -h ] -f FOLDER [ -p THREADS ] [ -o OUTFILE ] [ -j JSON ] [ -v ] spacesavers2_pdq: get quick per user info ( number of inodes and bytes ) . options: -h, --help show this help message and exit -f FOLDER, --folder FOLDER spacesavers2_pdq will be run on all inodes in this folder and its subfolders -p THREADS, --threads THREADS number of threads to be used ( default 4 ) -o OUTFILE, --outfile OUTFILE outfile ... by default output is printed to screen -j JSON, --json JSON outfile file in JSON format. -v, --version show program ' s version number and exit Version: v0.11.6 Example: > spacesavers2_pdq -f /path/to/folder -p 4 -o /path/to/output_file Output \u00b6 tab-delimited output (file) \u00b6 spacesavers2_pdq creates one tab seperated output line per user: % head -n1 test.out user1 1386138 6089531321856 user2 230616 2835680212992 user3 1499 126442496 The 3 items in the line are as follows: Column Description Example 1 username \"user1\" 2 total no. of inodes owned 1386138 3 total no. of bytes occupied 6089531321856 JSON output \u00b6 Here is an example output: { \"/path/to/some/folder \": { \"1234\": { \"username\": \"user1\", \"ninodes\": 1267, \"nbytes\": 96084992 }, \"4356\": { \"username\": \"user2\", \"ninodes\": 895, \"nbytes\": 89249280 } } } spacesavers2_pdq creates TSV (or JSON) file per-datamount per-run (typically per-date). If run daily, this soon creates a lot of files to keep track of. Hence, it is best to save the data in a sqlite db using: spacesavers2_pdq_create_db and spacesavers2_pdq_update_db","title":"pdq"},{"location":"pdq/#spacesavers2_pdq","text":"pdq = Pretty Darn Quick This uses glob library to list all files in a user-provided folder recursively. For each user it gathers information like: - total number of inodes - total number of bytes It is quick tool to gather datapoints to monitor filesystem usage. Typically, can be run once daily and compared with previous days run to find large changes.","title":"spacesavers2_pdq"},{"location":"pdq/#inputs","text":"--folder : Path to the folder to run spacesavers2_pdq on. --threads : spacesavers2_pdq uses multiprocessing library to parallelize orchestration. This defines the number of threads to run in parallel. --outfile : If not supplied then the output is written to the screen. --json : Optional, if provided output is also written in JSON format. NOTE: spacesavers2_pdq reports errors (eg. cannot read file) to STDERR usage: spacesavers2_pdq [ -h ] -f FOLDER [ -p THREADS ] [ -o OUTFILE ] [ -j JSON ] [ -v ] spacesavers2_pdq: get quick per user info ( number of inodes and bytes ) . options: -h, --help show this help message and exit -f FOLDER, --folder FOLDER spacesavers2_pdq will be run on all inodes in this folder and its subfolders -p THREADS, --threads THREADS number of threads to be used ( default 4 ) -o OUTFILE, --outfile OUTFILE outfile ... by default output is printed to screen -j JSON, --json JSON outfile file in JSON format. -v, --version show program ' s version number and exit Version: v0.11.6 Example: > spacesavers2_pdq -f /path/to/folder -p 4 -o /path/to/output_file","title":"Inputs"},{"location":"pdq/#output","text":"","title":"Output"},{"location":"pdq/#tab-delimited-output-file","text":"spacesavers2_pdq creates one tab seperated output line per user: % head -n1 test.out user1 1386138 6089531321856 user2 230616 2835680212992 user3 1499 126442496 The 3 items in the line are as follows: Column Description Example 1 username \"user1\" 2 total no. of inodes owned 1386138 3 total no. of bytes occupied 6089531321856","title":"tab-delimited output (file)"},{"location":"pdq/#json-output","text":"Here is an example output: { \"/path/to/some/folder \": { \"1234\": { \"username\": \"user1\", \"ninodes\": 1267, \"nbytes\": 96084992 }, \"4356\": { \"username\": \"user2\", \"ninodes\": 895, \"nbytes\": 89249280 } } } spacesavers2_pdq creates TSV (or JSON) file per-datamount per-run (typically per-date). If run daily, this soon creates a lot of files to keep track of. Hence, it is best to save the data in a sqlite db using: spacesavers2_pdq_create_db and spacesavers2_pdq_update_db","title":"JSON output"},{"location":"pdq_create_db/","text":"spacesavers2_pdq_create_db \u00b6 pdq = Pretty Darn Quick spacesavers2_pdq creates TSV (or JSON) file per-datamount per-run (typically per-date). If run daily, this soon creates a lot of files to keep track of. Hence, it is best to save the data in a sqlite db. This command create the basic schema for that db. The schema looks like this: Inputs \u00b6 --filepath : where to create the \".db\" file. --overwrite : toggle to overwrite if the \".db\" file already exists. usage: spacesavers2_pdq_create_db [ -h ] -f FILEPATH [ -o | --overwrite | --no-overwrite ] [ -v ] spacesavers2_pdq_create_db: create a sqlitedb file with the optimized schema. options: -h, --help show this help message and exit -f FILEPATH, --filepath FILEPATH spacesavers2_pdq_create_db will create this sqlitedb file -o, --overwrite, --no-overwrite overwrite output file if it already exists. Use this with caution as it will delete existing file and its contents!! -v, --version show program ' s version number and exit Version: v0.13.0-dev Example: > spacesavers2_pdq_create_db -f /path/to/sqlitedbfile Output \u00b6 db file \u00b6 sqlite \".db\" file with 4 tables % sqlite3 pdq.db SQLite version 3 .26.0 2018 -12-01 12 :34:55 Enter \".help\" for usage hints. sqlite> .table datamounts datapoints dates users sqlite> .schema CREATE TABLE users ( user_id INTEGER PRIMARY KEY, username TEXT NOT NULL, first_name TEXT NOT NULL, last_name TEXT NOT NULL ) ; CREATE TABLE dates ( date_int INTEGER PRIMARY KEY, date_text TEXT UNIQUE NOT NULL ) ; CREATE TABLE datamounts ( datamount_id INTEGER PRIMARY KEY, datamount_name TEXT UNIQUE NOT NULL ) ; CREATE TABLE datapoints ( datapoint_id INTEGER PRIMARY KEY, date_int INTEGER, datamount_id INTEGER, user_id INTEGER, ninodes INTEGER, nbytes INTEGER, FOREIGN KEY ( user_id ) REFERENCES users ( user_id ) , FOREIGN KEY ( datamount_id ) REFERENCES datamounts ( datamount_id ) , FOREIGN KEY ( date_int ) REFERENCES dates ( date_int ) ) ;","title":"pdq_create_db"},{"location":"pdq_create_db/#spacesavers2_pdq_create_db","text":"pdq = Pretty Darn Quick spacesavers2_pdq creates TSV (or JSON) file per-datamount per-run (typically per-date). If run daily, this soon creates a lot of files to keep track of. Hence, it is best to save the data in a sqlite db. This command create the basic schema for that db. The schema looks like this:","title":"spacesavers2_pdq_create_db"},{"location":"pdq_create_db/#inputs","text":"--filepath : where to create the \".db\" file. --overwrite : toggle to overwrite if the \".db\" file already exists. usage: spacesavers2_pdq_create_db [ -h ] -f FILEPATH [ -o | --overwrite | --no-overwrite ] [ -v ] spacesavers2_pdq_create_db: create a sqlitedb file with the optimized schema. options: -h, --help show this help message and exit -f FILEPATH, --filepath FILEPATH spacesavers2_pdq_create_db will create this sqlitedb file -o, --overwrite, --no-overwrite overwrite output file if it already exists. Use this with caution as it will delete existing file and its contents!! -v, --version show program ' s version number and exit Version: v0.13.0-dev Example: > spacesavers2_pdq_create_db -f /path/to/sqlitedbfile","title":"Inputs"},{"location":"pdq_create_db/#output","text":"","title":"Output"},{"location":"pdq_create_db/#db-file","text":"sqlite \".db\" file with 4 tables % sqlite3 pdq.db SQLite version 3 .26.0 2018 -12-01 12 :34:55 Enter \".help\" for usage hints. sqlite> .table datamounts datapoints dates users sqlite> .schema CREATE TABLE users ( user_id INTEGER PRIMARY KEY, username TEXT NOT NULL, first_name TEXT NOT NULL, last_name TEXT NOT NULL ) ; CREATE TABLE dates ( date_int INTEGER PRIMARY KEY, date_text TEXT UNIQUE NOT NULL ) ; CREATE TABLE datamounts ( datamount_id INTEGER PRIMARY KEY, datamount_name TEXT UNIQUE NOT NULL ) ; CREATE TABLE datapoints ( datapoint_id INTEGER PRIMARY KEY, date_int INTEGER, datamount_id INTEGER, user_id INTEGER, ninodes INTEGER, nbytes INTEGER, FOREIGN KEY ( user_id ) REFERENCES users ( user_id ) , FOREIGN KEY ( datamount_id ) REFERENCES datamounts ( datamount_id ) , FOREIGN KEY ( date_int ) REFERENCES dates ( date_int ) ) ;","title":"db file"},{"location":"pdq_update_db/","text":"spacesavers2_pdq_update_db \u00b6 pdq = Pretty Darn Quick spacesavers2_pdq creates TSV (or JSON) file per-datamount per-run (typically per-date). If run daily, this soon creates a lot of files to keep track of. Hence, it is best to save the data in a sqlite db. spacesavers2_pdq_create_db command creates the basic schema for that db. Then this command can be used to populate the database. Inputs \u00b6 --tsv : .tsv or .tsv.gz created using spacesavers2_pdq --database : .db file created using spacesavers2_pdq_create_db --datamount : eg. CCBR or CCBR_Pipeliner --date : integer date in YYYYMMDD format usage: spacesavers2_pdq_update_db [ -h ] -t TSV -o DATABASE -m DATAMOUNT -d DATE [ -v ] spacesavers2_pdq_create_db: update/append date from TSV to DB file options: -h, --help show this help message and exit -t TSV, --tsv TSV spacesavers2_pdq output TSV file -o DATABASE, --database DATABASE database file path ( use spacesavers2_pdb_create_db to create if it does not exists. ) -m DATAMOUNT, --datamount DATAMOUNT name of the datamount eg. CCBR or CCBR_Pipeliner -d DATE, --date DATE date in YYYYMMDD integer format -v, --version show program ' s version number and exit Version: v0.13.0-dev Example: > spacesavers2_pdq_update_db -t /path/to/tsv -o /path/to/db -m datamount_name -d date Output \u00b6 updated db file \u00b6 sqlite \".db\" file with 4 tables is updated. NOTE: new users are automatically added to \"users\" table new datemounts are automatically added to \"datamounts\" table new dates are automatically added to \"dates\" table if >0 datapoints exist in the \".db\" for a (date + datamount) combination then warning is displayed and no data is appended","title":"pdq_update_db"},{"location":"pdq_update_db/#spacesavers2_pdq_update_db","text":"pdq = Pretty Darn Quick spacesavers2_pdq creates TSV (or JSON) file per-datamount per-run (typically per-date). If run daily, this soon creates a lot of files to keep track of. Hence, it is best to save the data in a sqlite db. spacesavers2_pdq_create_db command creates the basic schema for that db. Then this command can be used to populate the database.","title":"spacesavers2_pdq_update_db"},{"location":"pdq_update_db/#inputs","text":"--tsv : .tsv or .tsv.gz created using spacesavers2_pdq --database : .db file created using spacesavers2_pdq_create_db --datamount : eg. CCBR or CCBR_Pipeliner --date : integer date in YYYYMMDD format usage: spacesavers2_pdq_update_db [ -h ] -t TSV -o DATABASE -m DATAMOUNT -d DATE [ -v ] spacesavers2_pdq_create_db: update/append date from TSV to DB file options: -h, --help show this help message and exit -t TSV, --tsv TSV spacesavers2_pdq output TSV file -o DATABASE, --database DATABASE database file path ( use spacesavers2_pdb_create_db to create if it does not exists. ) -m DATAMOUNT, --datamount DATAMOUNT name of the datamount eg. CCBR or CCBR_Pipeliner -d DATE, --date DATE date in YYYYMMDD integer format -v, --version show program ' s version number and exit Version: v0.13.0-dev Example: > spacesavers2_pdq_update_db -t /path/to/tsv -o /path/to/db -m datamount_name -d date","title":"Inputs"},{"location":"pdq_update_db/#output","text":"","title":"Output"},{"location":"pdq_update_db/#updated-db-file","text":"sqlite \".db\" file with 4 tables is updated. NOTE: new users are automatically added to \"users\" table new datemounts are automatically added to \"datamounts\" table new dates are automatically added to \"dates\" table if >0 datapoints exist in the \".db\" for a (date + datamount) combination then warning is displayed and no data is appended","title":"updated db file"},{"location":"usurp/","text":"spacesavers2_usurp \u00b6 This takes in the TSV file generated by spacesavers2_grubbers and a hash from its first column to: find the row corresponding to the hash keep one copy as of the duplicates as \"original copy\" delete other copies and replace them with hard (or soft) links Deleting these high-value duplicates has the biggest impact on the users overall digital footprint. Inputs \u00b6 --grubber output file from spacesavers2_grubbers . --hash a hash from its first column of the grubber TSV. --force (OPTIONAL) if the duplicates are cross-device then hard links cannot be made, with --force you can force using sym-links instead. The GRUBBER file has the following columns: | Column | Description | | ------ | ------------------------------------- | | 1 | combined hash | | 2 | number of duplicates found | | 3 | total size of all duplicates (human readable) | | 4 | size of each duplicate (human readable) | | 5 | original file | | 6 | \";\"-separated list of duplicates files | usage: spacesavers2_usurp [ -h ] -g GRUBBER -x HASH [ -f | --force | --no-force ] spacesavers2_usurp: delete all but one copy of the file matching the hash and replace all other copies with hardlinks options: -h, --help show this help message and exit -g GRUBBER, --grubber GRUBBER spacesavers2_grubbers output TSV file -x HASH, --hash HASH hash ( or unique partial hash ) from column 1 of spacesavers2_grubbers TSV file -f, --force, --no-force forcefully create symlink if hardlink is not possible Version: v0.8 Example: > spacesavers2_usurp -g grubbers.TSV -h someHash Outputs \u00b6 On screen confirmation that the duplicates are replaced with hard (or soft) links.","title":"usurp"},{"location":"usurp/#spacesavers2_usurp","text":"This takes in the TSV file generated by spacesavers2_grubbers and a hash from its first column to: find the row corresponding to the hash keep one copy as of the duplicates as \"original copy\" delete other copies and replace them with hard (or soft) links Deleting these high-value duplicates has the biggest impact on the users overall digital footprint.","title":"spacesavers2_usurp"},{"location":"usurp/#inputs","text":"--grubber output file from spacesavers2_grubbers . --hash a hash from its first column of the grubber TSV. --force (OPTIONAL) if the duplicates are cross-device then hard links cannot be made, with --force you can force using sym-links instead. The GRUBBER file has the following columns: | Column | Description | | ------ | ------------------------------------- | | 1 | combined hash | | 2 | number of duplicates found | | 3 | total size of all duplicates (human readable) | | 4 | size of each duplicate (human readable) | | 5 | original file | | 6 | \";\"-separated list of duplicates files | usage: spacesavers2_usurp [ -h ] -g GRUBBER -x HASH [ -f | --force | --no-force ] spacesavers2_usurp: delete all but one copy of the file matching the hash and replace all other copies with hardlinks options: -h, --help show this help message and exit -g GRUBBER, --grubber GRUBBER spacesavers2_grubbers output TSV file -x HASH, --hash HASH hash ( or unique partial hash ) from column 1 of spacesavers2_grubbers TSV file -f, --force, --no-force forcefully create symlink if hardlink is not possible Version: v0.8 Example: > spacesavers2_usurp -g grubbers.TSV -h someHash","title":"Inputs"},{"location":"usurp/#outputs","text":"On screen confirmation that the duplicates are replaced with hard (or soft) links.","title":"Outputs"}]}